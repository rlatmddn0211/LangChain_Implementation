{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "chat=ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "t=PromptTemplate(\n",
    "    template=\"What is the capital of {country}\",\n",
    "    input_variables=[\"country\"],\n",
    "\n",
    ")\n",
    "t.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***FewShotPromptTemplate***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France is a country located in Western Europe. It is known for its rich history, culture, and cuisine. The capital city is Paris, which is famous for landmarks such as the Eiffel Tower and the Louvre Museum. France is also known for its wine production, fashion industry, and art scene. The country has a diverse landscape, including mountains, beaches, and countryside. French is the official language, and the currency is the Euro. France is a member of the European Union and is one of the most visited countries in the world."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'France is a country located in Western Europe. It is known for its rich history, culture, and cuisine. The capital city is Paris, which is famous for landmarks such as the Eiffel Tower and the Louvre Museum. France is also known for its wine production, fashion industry, and art scene. The country has a diverse landscape, including mountains, beaches, and countryside. French is the official language, and the currency is the Euro. France is a member of the European Union and is one of the most visited countries in the world.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples=[\n",
    "    {\n",
    "        \"question\":\"What do you know about France?\",\n",
    "        \"answer\":\"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: Frence\n",
    "        Food: Wine and Cheese\n",
    "        Currency:Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\":\"What do you know about Italy?\",\n",
    "        \"answer\":\"\"\"\n",
    "        I know this:\n",
    "        Capital:Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\":\"What do you know about Greece?\",\n",
    "        \"answer\":\"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "chat.predict(\"what do you know about France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "I know the following:\n",
      "Capital: Seoul\n",
      "Language: Korean\n",
      "Food: Kimchi and Bibimbap\n",
      "Currency: South Korean Won"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI: \\nI know the following:\\nCapital: Seoul\\nLanguage: Korean\\nFood: Kimchi and Bibimbap\\nCurrency: South Korean Won')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_template=\"\"\"\n",
    "    Human:{question}\n",
    "    AI:{answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt=PromptTemplate.from_template(example_template)\n",
    "\n",
    "prompt=FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")\n",
    "\n",
    "chain=prompt|chat\n",
    "chain.invoke({\n",
    "    \"country\":\"Korea\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***FewShotChatMessagePromptTemplate***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=[\n",
    "    {\n",
    "        \"country\":\"France\",\n",
    "        \"answer\":\"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: Frence\n",
    "        Food: Wine and Cheese\n",
    "        Currency:Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\":\"Italy\",\n",
    "        \"answer\":\"\"\"\n",
    "        I know this:\n",
    "        Capital:Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\":\"Greece\",\n",
    "        \"answer\":\"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        I know this:\n",
      "        There are two Koreas - North Korea and South Korea\n",
      "        Capital of South Korea: Seoul\n",
      "        Language: Korean\n",
      "        Food: Kimchi and Bibimbap\n",
      "        Currency: South Korean Won"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n        I know this:\\n        There are two Koreas - North Korea and South Korea\\n        Capital of South Korea: Seoul\\n        Language: Korean\\n        Food: Kimchi and Bibimbap\\n        Currency: South Korean Won')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "example_prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"human\",\"What do you know about {country}\"),\n",
    "    (\"ai\",\"{answer}\")\n",
    "])\n",
    "\n",
    "example_prompt=FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "final_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",\"You are a geography expert, you give short answers\"),\n",
    "    example_prompt,\n",
    "    (\"human\",\"What do you know about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=final_prompt|chat\n",
    "chain.invoke({\"country\": \"Korea\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LengBasedExampleSelector***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=[\n",
    "    {\n",
    "        \"question\":\"What do you know about France?\",\n",
    "        \"answer\":\"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: Frence\n",
    "        Food: Wine and Cheese\n",
    "        Currency:Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\":\"What do you know about Italy?\",\n",
    "        \"answer\":\"\"\"\n",
    "        I know this:\n",
    "        Capital:Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\":\"What do you know about Greece?\",\n",
    "        \"answer\":\"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self,examples):\n",
    "        self.examples=examples\n",
    "    def add_example(self,example):\n",
    "        self.examples.append(example)\n",
    "    def select_examples(self,input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Human:What do you know about Greece?\\n    AI:\\n        I know this:\\n        Capital: Athens\\n        Language: Greek\\n        Food: Souvlaki and Feta Cheese\\n        Currency: Euro\\n        \\n\\n\\nHuman: What do you know about Germany?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_template=\"\"\"\n",
    "    Human:{question}\n",
    "    AI:{answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt=PromptTemplate.from_template(example_template)\n",
    "\n",
    "# example_selector=LengthBasedExampleSelector(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     max_length=180,\n",
    "# )\n",
    "\n",
    "example_selector=RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt=FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to load prompt templates from DISK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Germany'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt=load_prompt(\"./prompt.json\")\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Korea'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "prompt=load_prompt(\"./prompt.yaml\")\n",
    "prompt.format(country=\"Korea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to compose prompts?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    \\n    You are a role playing assistant.\\n    And you are impersonating a Programmer\\n\\n\\n    \\n    This is an example of how you talk:\\n\\n    Human: What is your role?\\n    You: I am an AI engineer\\n\\n\\n    \\n    Start now!\\n\\n    Human: What is your specific research field?\\n    You:\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate #많은 프롬프트들을 하나로 묶어주는 역할\n",
    "\n",
    "\n",
    "chat=ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt=[\n",
    "    (\"intro\",intro),\n",
    "    (\"example\",example),\n",
    "    (\"start\",start),\n",
    "]\n",
    "\n",
    "full_prompt=PipelinePromptTemplate(final_prompt=final,pipeline_prompts=prompt)\n",
    "\n",
    "full_prompt.format(\n",
    "    character=\"Programmer\",\n",
    "    example_question=\"What is your role?\",\n",
    "    example_answer=\"I am an AI engineer\",\n",
    "    question=\"What is your specific research field?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I specialize in machine learning and natural language processing. My research focuses on developing algorithms and models that can understand and generate human language."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='I specialize in machine learning and natural language processing. My research focuses on developing algorithms and models that can understand and generate human language.')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=full_prompt|chat\n",
    "chain.invoke({\n",
    "    \"character\":\"Programmer\",\n",
    "    \"example_question\":\"What is your role?\",\n",
    "    \"example_answer\":\"I am an AI engineer\",\n",
    "    \"question\":\"What is your specific research field?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CLIP (Contrastive Language-Image Pre-training) is a vision-language model developed by OpenAI that is capable of understanding and generating text about images in a way that is similar to how humans do. The model is trained on a large dataset of images and their corresponding text descriptions, allowing it to learn the relationship between visual and textual information.\\n\\nCLIP is unique in that it does not require any explicit alignment between images and text during training. Instead, it learns to associate images and text by maximizing the similarity between them in a process known as contrastive learning. This allows the model to generalize to new images and text descriptions that it has not seen before.\\n\\nOne of the key advantages of CLIP is its ability to perform a wide range of vision-language tasks, such as image classification, object detection, and image captioning, without the need for task-specific training. This makes it a versatile and powerful tool for a variety of applications in computer vision and natural language processing.\\n\\nOverall, CLIP represents a significant advancement in the field of vision-language models and has the potential to revolutionize how machines understand and interact with visual and textual information.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache,set_debug\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "chat=ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "chat.predict(\"Could you explain me about CLIP, which is a vision-language Model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CLIP (Contrastive Language-Image Pre-training) is a vision-language model developed by OpenAI that is capable of understanding and generating text about images in a way that is similar to how humans do. The model is trained on a large dataset of images and their corresponding text descriptions, allowing it to learn the relationship between visual and textual information.\\n\\nCLIP is unique in that it does not require any explicit alignment between images and text during training. Instead, it learns to associate images and text by maximizing the similarity between them in a process known as contrastive learning. This allows the model to generalize to new images and text descriptions that it has not seen before.\\n\\nOne of the key advantages of CLIP is its ability to perform a wide range of vision-language tasks, such as image classification, object detection, and image captioning, without the need for task-specific training. This makes it a versatile and powerful tool for a variety of applications in computer vision and natural language processing.\\n\\nOverall, CLIP represents a significant advancement in the field of vision-language models and has the potential to revolutionize how machines understand and interact with visual and textual information.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"Could you explain me about CLIP, which is a vision-language Model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in ConsoleCallbackHandler.on_llm_start callback: 1 validation error for Run\n",
      "name\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "Error in ConsoleCallbackHandler.on_llm_end callback: TracerException('No LLM Run found to be traced for 2a185ed4-811f-4a3f-8c2b-06fd83f9e7b4')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CLIP (Contrastive Language-Image Pre-training) is a vision-language model developed by OpenAI that is capable of understanding and generating text about images in a way that is similar to how humans do. It is a large-scale neural network that has been trained on a diverse range of images and text data from the internet.\\n\\nCLIP is unique in that it is trained to understand both images and text simultaneously, allowing it to perform a wide range of tasks such as image classification, image generation, and image-text matching. This means that it can generate accurate descriptions of images, classify images into different categories, and even generate images based on textual descriptions.\\n\\nOne of the key features of CLIP is its ability to understand and generate text in a way that is contextually relevant to the images it is analyzing. This allows it to perform tasks such as zero-shot image classification, where it can accurately classify images into categories that it has never seen before.\\n\\nOverall, CLIP represents a significant advancement in the field of vision-language models and has the potential to revolutionize how we interact with and understand visual data.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache,set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "set_debug(True)\n",
    "\n",
    "chat=ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "chat.predict(\"Could you explain me about CLIP, which is a vision-language Model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**만약 cache를 메모리가 아닌 데이터베이스로 비휘발성으로 관리하고 싶다면?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in ConsoleCallbackHandler.on_llm_start callback: 1 validation error for Run\n",
      "name\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "Error in ConsoleCallbackHandler.on_llm_end callback: TracerException('No LLM Run found to be traced for f9e39f28-0923-4496-bb48-83c8d4945370')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CLIP (Contrastive Language-Image Pre-training) is a vision-language model developed by OpenAI that is capable of understanding both images and text. It is trained on a large dataset of images and their corresponding captions, allowing it to learn to associate images with their descriptions.\\n\\nOne of the key features of CLIP is its ability to perform zero-shot learning, meaning it can recognize objects and concepts in images that it has never seen before by understanding the text associated with the image. This is achieved through a process called contrastive learning, where the model is trained to distinguish between correct image-text pairs and incorrect pairs.\\n\\nCLIP has shown impressive performance on a wide range of vision-language tasks, such as image classification, object detection, and image generation. It has also demonstrated strong generalization capabilities, outperforming other vision-language models on various benchmarks.\\n\\nOverall, CLIP represents a significant advancement in the field of vision-language modeling and has the potential to be applied to a wide range of real-world applications, such as image search, content moderation, and accessibility tools for the visually impaired.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "chat=ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "chat.predict(\"Could you explain me about CLIP, which is a vision-language Model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Serialization & Cost Trace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 169\n",
      "\tPrompt Tokens: 21\n",
      "\tCompletion Tokens: 148\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0003275\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "chat=ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "with get_openai_callback() as usage:\n",
    "    chat.invoke(\"What are the SOTA models for Computer Vision, and NLP?\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Serialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungwookim/Desktop/FULLSTCK-GPT/env/lib/python3.10/site-packages/langchain/utils/utils.py:159: UserWarning: WARNING! temperatue is not default parameter.\n",
      "                temperatue was transferred to model_kwargs.\n",
      "                Please confirm that temperatue is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "chat=OpenAI(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    temperatue=0.1,\n",
    "    max_tokens=450,\n",
    ")\n",
    "chat.save(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungwookim/Desktop/FULLSTCK-GPT/env/lib/python3.10/site-packages/langchain/llms/openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/seungwookim/Desktop/FULLSTCK-GPT/env/lib/python3.10/site-packages/langchain/llms/openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIChat(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-16k', model_kwargs={'temperature': 0.7, 'max_tokens': 450, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}, 'temperatue': 0.1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "chat=load_llm(\"model.json\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
