{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.schema import StrOutputParser\n",
    "chat=ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(BaseOutputParser):\n",
    "    def parse(self,text):\n",
    "        items=text.split(\"-\")\n",
    "        parsed_result={}\n",
    "        for item in items:\n",
    "            if not item.strip():\n",
    "                continue\n",
    "            if \":\" in item:\n",
    "                key,value=item.split(\":\",1)\n",
    "                parsed_result[key.strip()]=value.strip()\n",
    "        return parsed_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_encoder_prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a ViT (Vision Transformer), and you are going to receive a prompt as an image description. Let's assume that this prompt is an image input. You have to turn this image into a vision feature. The output will be sent to LLM decoder.\"),\n",
    "    (\"human\",\"{description}\")\n",
    "])\n",
    "\n",
    "vision_encoder_chain=vision_encoder_prompt|chat|StrOutputParser()|CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_decoder_prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a image captioning model. You will receive a dictionary of vision feature from vision encoder, and what you have to do is create a warm explanation for the blind. Based on the vision features, you have to create a description about the scene for the blind.\"),\n",
    "    (\"human\",\"Here are the visual features extracted from an image {visual_features}\"),\n",
    "])\n",
    "llm_decoder_chain=llm_decoder_prompt|chat|StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The ViT processes the image input and extracts the following vision feature:]\n",
      "\n",
      "- Scene: Heavy downpour in a city at night\n",
      "- Elements: Wet asphalt reflecting pink and blue neon signs of shops, cars rushing by with long trails of red taillightsImagine standing in the middle of a bustling city on a rainy night. The sound of raindrops hitting the ground fills the air as you feel the wet asphalt beneath your feet. The neon signs of shops around you cast a pink and blue glow, reflecting off the shiny surface of the road. Cars speed past, leaving behind long trails of red taillights that blur together in the downpour. Despite the darkness of the night, the city is alive with movement and color, creating a vibrant and dynamic scene."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Imagine standing in the middle of a bustling city on a rainy night. The sound of raindrops hitting the ground fills the air as you feel the wet asphalt beneath your feet. The neon signs of shops around you cast a pink and blue glow, reflecting off the shiny surface of the road. Cars speed past, leaving behind long trails of red taillights that blur together in the downpour. Despite the darkness of the night, the city is alive with movement and color, creating a vibrant and dynamic scene.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mllm_chain={\"visual_features\":vision_encoder_chain}|llm_decoder_chain\n",
    "mllm_chain.invoke({\n",
    "    \"description\": \"A heavy downpour in a city at night. The wet asphalt reflects the pink and blue neon signs of the shops. Cars are rushing by, leaving long trails of red taillights.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
